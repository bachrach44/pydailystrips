import sys, getopt, time, urllib2, re, argparse

#TODO: 
#alt tags?
#check version, link to update if not up to date

outfile = ""

def classType(name, data, cl):
	newstrip = {}
	#copy data from cl to newstrip
	if (not cl):
		return "can't find class %s for strip %s<br /><br />\n\n" %(data.get('useclass'), name)
	for v in cl:
		#variables in class def
		if ("$" in cl[v]):
			for val in data:
				newstrip[v] = str.replace(cl[v], "$"+val, data[val])
		else:
			newstrip[v] = cl[v]
	for val in data:
		#variables in strip def
		if ("$" in val):
			for v in newstrip:
				newstrip[v] = str.replace(newstrip[v], val, data[val])
		else:
			#add in props from strip that don't exist in class. Overwright if they do
			newstrip[val] = data[val]
	return stripType(name, newstrip)
	
def getArtist(data):
	if ('artist' in data):
		return ' by %s' %(data['artist'])
	else:
		return ''

def getTitle(data):
	txt = ''
	if (data.get('homepage')):
		txt +='<b><a href=\"%s\">%s</a></b>' %(stringsubs(data.get('homepage')), data.get('name'))
	else:
		txt += '<b>%s</b>' %(stringsubs(str(data.get('name'))))
	return txt

def stripType(name, data):
	html = getTitle(data)+getArtist(data)+"<br />\n"
	if (data['type'].startswith('search')):
		url = data.get('homepage')
		if (data.get('searchpage')):
			url = data.get('searchpage')
		url = stringsubs(url)
		request = urllib2.Request(url)
		request.add_header("User-Agent", "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/37.0.2062.120 Chrome/37.0.2062.120 Safari/537.36") 
		request.add_header("Accept", "text/html")
		request.add_header('Pragma', 'no-cache')
		request.add_header('Cache-Control', 'no-cache')
		request.add_header('Accept-Language', 'en-US,en;q=0.8')
		if (data.get('referer')):
			request.add_header("Referer", data.get('referer'))
		try:
			resp = urllib2.urlopen(request)
			page = resp.read()
			pattern = re.compile(data.get('searchpattern'))
			m = pattern.search(page)
			if (m):
				html += "<img src=\""
				if (data.get('baseurl')):
					html += str(data.get('baseurl'))+m.group()
				else:
					html += m.group()
				html += "\" /><br /><br />\n\n"
			else:
				html += 'Could not find regex in page %s<br /><br />\n' %(url)
		except urllib2.HTTPError, err:
			if err.code == 404:
				html += '404 error trying to access %s.<br /><br />\n\n' %(url)
			else:
				html += 'error when trying to get %s. Server returned a %s error which means %s<br /><br />\n\n' %(url, err.code, err.reason)
		except:
			html += 'an unknown error occured while trying to access %s<br /><br/>\n\n' %(url)
	elif (data['type'].startswith('generate')):
		image = data['imageurl']
		html += "<img src=\""+stringsubs(image)+"\" /><br />\n\n"
	else:
		return html+"Say what? Couldn't understand the strip type<br />\n"
	return html

def stringsubs(image):
	image = str.replace(image, "%Y",time.strftime("%Y"))
	image = str.replace(image, "%y",time.strftime("%y"))
	image = str.replace(image, "%m",time.strftime("%m"))
	image = str.replace(image, "%d",time.strftime("%d"))
	return image

def getStrips(toget, strips):
	for s in toget:
		if (s in strips):
			strip = strips.get(s)
			if ('useclass' in strip):
				html = classType(s, strip, strips.get(strip['useclass']))
			elif('type' in strip):
				html = stripType(s, strip)
			else:
				html = "could not understand strip definition.<br />\n\n"		
			with open(outfile, "a") as myfile:
				myfile.write(html)
		else:
			with open(outfile, "a") as myfile:
				myfile.write('Could not find strip %s in strips.def file<br />\n\n' %(s))
				print 'Could not find strip %s in strips.def file' %(s)
def init():
	strips = {}
	with open("strips.def", "r") as defs:
		strip = {}
		title = ''
    		for line in defs:
			line = line.strip()
			if (not line):
				continue
			if (line.startswith('#')):
				continue
			if (line.startswith("end")):
				strips[title] = strip
				strip = {}
				title=''
				continue
			if (line.startswith("strip")):
				id, name = line.split(" ", 1)
				if (strips.get(name)):
					print 'found a duplicate for %s in def file. Going with first one.' %(name)
					continue
				title = name
				strip['strip'] = name
				#print name
			elif (line.startswith("class")):
				id, call = line.split(" ", 1)
				title = call
			else:
				prop, val = line.split(" ", 1)
				strip[prop] = val
	return strips

def printFooter():
	with open(outfile, "a") as myfile:
		myfile.write("generated at "+time.asctime(time.localtime(time.time()))+"<br />\n")
		myfile.write("powered by <a href=\"https://github.com/bachrach44/pydailystrips\">py daily strips</a>\n</body></html>\n")

def printHeader():
	with open(outfile, "w") as myfile:
		myfile.write("<html><head><title>Python Daily Strips</title></head>\n<body><b><font size=4><center>Python Daily Strips for %s, %s</center></font></b><br /><br />\n\n" %(time.strftime("%A"), time.strftime("%b %d %Y")))

def loadFromFile(filename):
	with open (filename, "r") as myfile:
		text = myfile.read()
	return text.split()

def main(argv):
	#will call app with list of strips and possible opts
	#process some options (output, input)
	infile = ""
	global outfile
	outfile = "index.html"
	parser = argparse.ArgumentParser()
	parser.add_argument("-o", "--output", help="output file to write to")
	parser.add_argument("-i", "--input", help="input file to read from")
	parser.add_argument("-v", "--version", help="print version and exit", action="store_true")
	parser.add_argument("strips", metavar='strips', nargs='*', help='strips to load, separated by spaces')
	args = parser.parse_args()
	if (args.version):
		print "Are you kidding? This isn't even beta. What are you even doing looking at this?"
		exit(0)
	if (args.input):
		#print args.input
    		infile = args.input
	if (args.output):
		print "out"
		outfile = args.output
	#load classes from def file
	strips = init()

	toget = []
	if (infile):
		toget = loadFromFile(infile)
	else:
		#load toget from command line
		toget = args.strips

	printHeader()
	getStrips(toget, strips)
	printFooter()

main(sys.argv)

#other options I might consider supporting (from dailystrips)
#   quiet|q  verbose   local|l   noindex   archive|a   dailydir|d   stripdir   save|s
#   nostale   date=s   new|n   defs=s   nopersonal   basedir=s   list   proxy=s
#   proxyauth=s   noenvproxy   nospaces   useragent=s   random   nosystem   stripnav
#   nosymlinks   titles=s   retries=s   clean=s   updates=s   noupdates
#
#Options supported by dailystrips that I have no plan on supporting
#avantgo, lite





